{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/drive/1FlLpVn217ag8xYOcar_ygOjixxeVPn8A#scrollTo=QP-WtXKCzLx1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Продвинутый уровень понимания линейной регрессии\n",
    "\n",
    "Как понять, насколько хорошо наша модель \"выучилась\" по данным - насколько хорошо она попала в центр облака точек? Давайте введём какую-то меру качества модели - тогда мы сможем говорить, что модель обучается, когда растёт мера качества ( о метриках качества мы поговорим в следующем урове этого модуля), Процесс машинного обучения сводится к тому, чтобы по опыту (обучающей выборке) $D$ подобрать такие коэффициенты линейной регрессии, что мера качества $L$ будет максимальной. Хорошей мерой качества $L$ для задачи регрессии является среднее значение квадрата разности между фактическим значением $y$ и прогнозом $\\hat{y}$ - такая метрика называется *Mean Squared Error* (берём со знаком минус, т.к. при увеличении качества модели метрика должна расти).\n",
    "$$\n",
    "L(\\hat{y}, y) = -\\frac{1}{N}\\sum_{i=1}^{N}\\left(y_i - \\hat{y_i}\\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Линейная регрессия относится в задачам обучения с учителем: опыт $D$ (который в ML называют *обучающей выборкой*) в нашем случае - это набор пар $x_i, y_i$ таких, что\n",
    "\n",
    "$$\n",
    "D = \\{(x_i, y_i) \\}_{i=\\overline{1,N}}\n",
    "$$\n",
    "\n",
    "Где $y_i$ - это \"правильный\" ответ (значение переменной $y$) на обучающем примере $x_i$, а $N$ - количество обучающих примеров. В задаче линейной регрессии $y \\in R$, то есть предсказывать нужно  непрерывную величину (в отличие от задачи классификации, где предсказания должны быть дискретными)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый объект $x_i$ является совокупностью признаков $x_i^1,\\ldots, x_i^k$. Размерность признакового пространства может быть разной, т.е. $x_i \\in R^k$, где $k$ может принимать значения от 1 (в задаче прогнозирования роста человека по его весу) до десятков тысяч (например, в задаче анализа текстов). Вот, например, как выглядит регрессия в трёхмерном пространстве\n",
    "\n",
    "![3d_lin_reg](https://248006.selcdn.ru/public/Data-science-4/img/3d_lin_reg.png)\n",
    "\n",
    "В случае многомерной линейной регрессии для $n-1$ измерений нам нужно будет выучить не два коэффициента $a$ и $b$, а $n$ коэффициентов - по одному на каждую координату $x_i$ плюс один коэффициент-свободный член\n",
    "\n",
    "$$\n",
    "L(\\hat{y}, y) = \\frac{1}{N}\\sum_{i=1}^{N}\\left(y_i - \\hat{y_i}\\right)^2 = \\frac{1}{N}\\sum_{i=1}^{N}\\left(y_i -  \\sum_{j=1}^{n}w_jx_i^j\\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой формуле:\n",
    "\n",
    "* $N$ - размер обучающей выборки (собрали информацию по 124 домам: $N=124$)\n",
    "* $i$ - порядковый номер объекта из обучающей выборки\n",
    "* $y_i$ - значение целевого признака (например, стоимость дома или рост человека) у объекта под номером $i$\n",
    "* $\\hat{y}_i$ - значение целевого признака, которое предсказала линейная регрессия\n",
    "\n",
    "Дальше мы расписываем, как предсказание $\\hat{y}_i$ получается по фичам:\n",
    "$$\n",
    "\\hat{y}_i = \\sum_{j=1}^{n}w_jx_i^j\n",
    "$$\n",
    "\n",
    "в этой формуле\n",
    "\n",
    "* $n$ - количество фичей (если по площади дома предсказываем цену, то $n=1$, у нас одна фича \n",
    "* $w_j$ - коэффициент при фиче под номером $j$, который показывает силу влияния этой фици на целевую переменную например $w_1$ $=$ `сила влияния площади дома на его цену`\n",
    "* $x_i^j$ - фича под номером $j$ у объекта под номером $i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Говоря математическим языком, *задача линейной* регрессии заключается в том, чтобы восстановить функцию зависимости $y$ тот $x$ в виде *линейной комбинации* признаков объекта. Сами признаки называются *фичами* (как и в других задачах машинного обучения):\n",
    "$$\n",
    "\\forall x_i: \\hat{y_i} = w_0 + w_1x_i^1 + \\ldots + + w_nx_i^n = \\sum_{j=1}^{n}w_jx_i^j = \\overline{x}_i^T\\overline{w}\n",
    "$$\n",
    "\n",
    "Если перейти от одного объекта обучающей выборки ко всей выборке с множеством объектом, то формулу **MSE** удобно для дальнейших вычислений переписать в т.н. матричной форме (подробнее см. в [статье про линейную регрессию](https://ru.wikipedia.org/wiki/Линейная_регрессия) )\n",
    "$$\n",
    "Q_{\\text{emp}} = \\frac{1}{2N}\\sum_{i=1}^{N}(y_i - \\hat{y_i})^2 = \\frac{1}{2N}\\sum_{i=1}^{N}(y_i - \\overline{x}_i^T\\overline{w})^2 = \\frac{1}{2N}||\\overline{Y}-\\overline{X}^T\\overline{w}||^2 = \\frac{1}{2N}\\left(\\overline{Y}-\\overline{X}^T\\overline{w}\\right)^T\\left(\\overline{Y}-\\overline{X}^T\\overline{w}\\right)\n",
    "$$\n",
    "\n",
    "Такой вид функции потерь называется RSS - *resudal squares sum*, на русский переводится как *остаточная сумма квадратов*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой формуле:\n",
    "\n",
    "* $Y$ - вектор-столбец с истинными значениями целевой переменной $y$ размерности $N \\times 1$, где $N$ - размер обучающей выборки\n",
    "* $X$ - матрица с фичами размерости $N \\times n$, где $N$ - размер обучающей выборки, а $n$ - число фичей\n",
    "* $w$ - вектор коэффициентов линейной регрессии размерности $1 \\times n$, где $n$ - количество фичей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Где $\\hat{y_i}$ - ответ нашего алгоритма машинного обучения $h(x, \\theta)$ на примере $x_i$. Чем больше значение $L$ (т.е. чем ближе оно к нулю, т.к. берём со знаком минус), тем лучше наша модель повторяет опыт $X \\in m \\times n$ - матрицу, где m - количество примеров в обучающей выборке, а $m$ - размерность пространства признаков. $w$ - это вектор параметров модели, который хотим обучить.\n",
    "\n",
    "Значение коэффициентов линейной регрессии $w$, которые будут являться решением нашей задачи, можно найти аналитически:\n",
    "$$\n",
    "\\overline{w} = \\left(X^TX\\right)^{-1}X^T\\overline{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим, как обучить модель линейной регрессии, пользуясь только библиотечными функциями - имеенно их вы будете применять при решении реальных задач на работе\n",
    "\n",
    "Сначала реализуем вспомогательную функцию для печати чисел  питоновского типа *float* в красивом виде без большого количества знаков после запятой: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndprint(a, format_string ='{0:.2f}'):\n",
    "    \"\"\"Функция, которая распечатывает список в красивом виде\"\"\"\n",
    "    return [format_string.format(v,i) for i,v in enumerate(a)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем исходные данные - датасет с ценами на дома в Бостоне. Это стандартный датасет, который используется для демонстрации алгоритмов настолько часто, что включён прямо в исходный код библиотеки sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_boston\n\u001b[32m      3\u001b[39m boston_dataset = load_boston()\n\u001b[32m      5\u001b[39m features = boston_dataset.data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/py313/lib/python3.13/site-packages/sklearn/datasets/__init__.py:161\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name == \u001b[33m\"\u001b[39m\u001b[33mload_boston\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    111\u001b[39m     msg = textwrap.dedent(\n\u001b[32m    112\u001b[39m \u001b[38;5;250m        \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m        `load_boston` has been removed from scikit-learn since version 1.2.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    159\u001b[39m \u001b[33;03m        \"\"\"\u001b[39;00m\n\u001b[32m    160\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    163\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m()[name]\n",
      "\u001b[31mImportError\u001b[39m: \n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston_dataset = load_boston()\n",
    "\n",
    "features = boston_dataset.data\n",
    "y = boston_dataset.target\n",
    "\n",
    "print('Матрица Объекты X Фичи  (размерность): %s %s' % features.shape)\n",
    "print('\\nЦелевая переменная y (размерность): %s' % y.shape)\n",
    "\n",
    "\n",
    "# текстовое описание датасета  - распечатать, если интересно print('\\n',boston_dataset.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем работать с датасетом в котором 506 наблюдений. У каждого наблюдения 13 фичей. Таким образом наша модель - это вектор $w$ в котором 13 компонент $w_1,\\ldots,w_n$ - по одному коэффициенту на каждую фичу.\n",
    "\n",
    "Код для аналитического вычисления коэффициентов линейной регрессии по формуле $\\overline{w} = \\left(X^TX\\right)^{-1}X^T\\overline{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "\n",
    "# вычисляем к-ты линейной регрессии\n",
    "w_analytic = inv(\n",
    "    features.T.dot(features)\n",
    ").dot(\n",
    "    features.T\n",
    ").dot(\n",
    "    y\n",
    ")\n",
    "print(\"Аналитически определённые коэффициенты \\n%s\" % ndprint(w_analytic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы проверить, насколько хорошо мы реализовали аналитическое вычисление коэффициентов, воспользуемся готовой библиотечной реализацией. Библиотечная реализация вычисляет коэффициенты не с помощью перемножения матриц (как мы)ю, а с помощью *приближённых* [методов для численного решения](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lstsq.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# обучаем модель \"из коробки\"\n",
    "reg = LinearRegression().fit(features, y)\n",
    "print(\"Коэффициенты, вычисленные моделью sklearn \\n%s\" % ndprint(reg.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы реализовали обучение модели линейной регрессии на языке Python, причём полученные коэффициенты в целом совпадают с результатами, полученными с помощью класса `sklearn.linear_model.LinearRegression`, который вычисляет коэффициенты приближённо. На практике пользуются именно библиотечной функцией, потому что при наличии большого количества точке и большого количества фичей \"самописная\" реализация будет работать дольше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
